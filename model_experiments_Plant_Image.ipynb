{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_experiments_Plant_Image.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthew-e-thomas/Plant_Image_Project/blob/main/model_experiments_Plant_Image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcPZixKlV_J0"
      },
      "source": [
        "##Import Libraries, get paths ready, and clone PlantDoc Repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQoOUV00TEQp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "071bde6a-ea96-4e20-ef89-1c18dc53af46"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import IPython.display as display\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2fpGrkJ8p8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57bc3d0c-6d21-4b06-e248-4c580c95ba9b"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lw2tvg9Z6Gc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cef715d-713f-48da-a69c-d8cfef8bdbcd"
      },
      "source": [
        "!git clone https://github.com/matthew-e-thomas/PlantDoc-Dataset.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PlantDoc-Dataset'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 2661 (delta 18), reused 3 (delta 1), pack-reused 2628\u001b[K\n",
            "Receiving objects: 100% (2661/2661), 932.25 MiB | 45.04 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n",
            "Checking out files: 100% (2578/2578), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLKalZlqj7OM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee6bd76d-1203-44dc-e68b-7ce82eafda12"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckyXtgJDTJCX"
      },
      "source": [
        "#run this if cloning plantdoc github\n",
        "from pathlib import Path\n",
        "data_dir = Path.cwd() / 'PlantDoc-Dataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyryrnzpd321"
      },
      "source": [
        "train_dir = data_dir.joinpath('train')\n",
        "test_dir = data_dir.joinpath('test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FL_I0sccYoK"
      },
      "source": [
        "#CLASS_NAMES_TRAIN = [item.name for item in train_dir.glob('*')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeZU_RH_kRI8"
      },
      "source": [
        "#run this if using improved plantdoc dataset from your google drive\n",
        "# import os\n",
        "# os.chdir('/content/drive/My Drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9zheJuME3mY"
      },
      "source": [
        "The next few cells are for reading in the dataset as a tfrecords, if desired"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf-j6Kwskgbu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6cde383-a4f4-43ff-93ce-3294c80456e9"
      },
      "source": [
        "#!unzip PlantDoc.v1-resize-416x416.tfrecord.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  PlantDoc.v1-resize-416x416.tfrecord.zip\n",
            " extracting: train/leaves.tfrecord   \n",
            " extracting: train/leaves_label_map.pbtxt  \n",
            " extracting: test/leaves.tfrecord    \n",
            " extracting: test/leaves_label_map.pbtxt  \n",
            " extracting: README.roboflow.txt     \n",
            " extracting: README.dataset.txt      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNTkDho1vzIM"
      },
      "source": [
        "# filepaths = [\"train/leaves.tfrecord\"]\n",
        "# dataset = tf.data.TFRecordDataset(filepaths)\n",
        "# for item in dataset.take(5):\n",
        "#     print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5Azy0YzkrJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db3d25c3-7559-4f73-8f12-7bccacdd9651"
      },
      "source": [
        "# raw_image_dataset = tf.data.TFRecordDataset(['train/leaves.tfrecord'])\n",
        "\n",
        "# # Create a dictionary describing the features.\n",
        "# image_feature_description = {\n",
        "#     #'height': tf.io.FixedLenFeature([], tf.int64),\n",
        "#     #'width': tf.io.FixedLenFeature([], tf.int64),\n",
        "#     #'depth': tf.io.FixedLenFeature([], tf.int64),\n",
        "#     'image': tf.io.FixedLenFeature([], tf.string),\n",
        "#     'label': tf.io.FixedLenFeature([], tf.string),\n",
        "# }\n",
        "\n",
        "# def _parse_image_function(example_proto):\n",
        "#   # Parse the input tf.train.Example proto using the dictionary above.\n",
        "#   return tf.io.parse_single_example(example_proto, image_feature_description)\n",
        "\n",
        "# parsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n",
        "# parsed_image_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: {label: ()}, types: {label: tf.string}>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGeHKNcVGp3w"
      },
      "source": [
        "# for image_features in parsed_image_dataset.take(5):\n",
        "#   image_raw = image_features['image'].numpy()\n",
        "#   display.display(display.Image(data=image_raw))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-St9N8EKWKmf"
      },
      "source": [
        "### Create Train, Validation, and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF47T4z8bENS"
      },
      "source": [
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJnkWPi4aazT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bf762c6-3a21-46cc-d620-5cd2680853ba"
      },
      "source": [
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  train_dir,\n",
        "  validation_split=0.1,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  #label_mode=\"categorical\",\n",
        "  #class_names=CLASS_NAMES,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2340 files belonging to 27 classes.\n",
            "Using 2106 files for training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2YKauriCbXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6642b5e1-f4ae-4556-d177-7d38a4c6098b"
      },
      "source": [
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  train_dir,\n",
        "  validation_split=0.1,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  #label_mode=\"categorical\",\n",
        "  #class_names=CLASS_NAMES,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2340 files belonging to 27 classes.\n",
            "Using 234 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9x8OeRBfErR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715c2eaf-dec5-4eaa-a75e-e570f2bae6ab"
      },
      "source": [
        "# class names\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)\n",
        "print(len(class_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Apple Scab Leaf', 'Apple leaf', 'Apple rust leaf', 'Bell_pepper leaf', 'Bell_pepper leaf spot', 'Blueberry leaf', 'Cherry leaf', 'Corn Gray leaf spot', 'Corn leaf blight', 'Corn rust leaf', 'Peach leaf', 'Potato leaf early blight', 'Potato leaf late blight', 'Raspberry leaf', 'Soyabean leaf', 'Squash Powdery mildew leaf', 'Strawberry leaf', 'Tomato Early blight leaf', 'Tomato Septoria leaf spot', 'Tomato leaf', 'Tomato leaf bacterial spot', 'Tomato leaf late blight', 'Tomato leaf mosaic virus', 'Tomato leaf yellow virus', 'Tomato mold leaf', 'grape leaf', 'grape leaf black rot']\n",
            "27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmSrK-xXbGwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d9a576-4d7d-482d-de2d-ea345d99b721"
      },
      "source": [
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  test_dir,\n",
        "  seed=123,\n",
        "  #label_mode=\"categorical\",\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 236 files belonging to 27 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T_tZq2lWQLV"
      },
      "source": [
        "### Create and Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okk1Hl4QFLaU"
      },
      "source": [
        "Please note that to save space, most of the trials we did experimenting with various optimizers or learning rates are not shown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c7RRufobngy"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import inception_resnet_v2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emo6i_l9ebtf"
      },
      "source": [
        "from keras.applications import InceptionResNetV2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usPAPbsQbpJA"
      },
      "source": [
        "def preprocess(image, label):\n",
        "    #resized_image = tf.image.resize(image, [224, 224])\n",
        "    final_image = inception_resnet_v2.preprocess_input(image)\n",
        "    return final_image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkwth-RkEDdS"
      },
      "source": [
        "\n",
        "train_set = train_ds.map(preprocess).prefetch(1)\n",
        "valid_set = val_ds.map(preprocess).prefetch(1)\n",
        "test_set = test_ds.map(preprocess).prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJYUG2iT48D9"
      },
      "source": [
        "#regularization with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "monitor_val_acc = EarlyStopping(monitor = 'val_loss', patience=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pWB_dzOdTI3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b312ce1-5fba-4bfe-c52d-476d3d485337"
      },
      "source": [
        "tf.random.set_seed(123)\n",
        "np.random.seed(123)\n",
        "base_model = InceptionResNetV2(weights=\"imagenet\", include_top=False)\n",
        "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "#x = keras.layers.Dense(1024, activation='relu')(avg)\n",
        "output = keras.layers.Dense(len(class_names), activation=\"softmax\")(avg)\n",
        "model = keras.models.Model(inputs=base_model.input, outputs=output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BxRXy-QvyAi"
      },
      "source": [
        "adagrad = tf.keras.optimizers.Adagrad()\n",
        "nadam = tf.keras.optimizers.Nadam(.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yeB8TCHenFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5806feff-9fb7-4d3b-e18f-7a8ce9916f0c"
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=nadam, metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=40, callbacks=monitor_val_acc)\n",
        "score = model.evaluate(test_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "66/66 [==============================] - 48s 733ms/step - loss: 2.3895 - accuracy: 0.3310 - val_loss: 1.7927 - val_accuracy: 0.4658\n",
            "Epoch 2/40\n",
            "66/66 [==============================] - 46s 691ms/step - loss: 0.8828 - accuracy: 0.7588 - val_loss: 1.4087 - val_accuracy: 0.5897\n",
            "Epoch 3/40\n",
            "66/66 [==============================] - 46s 703ms/step - loss: 0.2732 - accuracy: 0.9335 - val_loss: 1.4097 - val_accuracy: 0.5812\n",
            "Epoch 4/40\n",
            "66/66 [==============================] - 46s 700ms/step - loss: 0.1084 - accuracy: 0.9763 - val_loss: 1.4665 - val_accuracy: 0.5940\n",
            "Epoch 5/40\n",
            "66/66 [==============================] - 46s 702ms/step - loss: 0.0717 - accuracy: 0.9801 - val_loss: 1.5499 - val_accuracy: 0.6111\n",
            "Epoch 6/40\n",
            "66/66 [==============================] - 46s 703ms/step - loss: 0.0627 - accuracy: 0.9815 - val_loss: 1.5706 - val_accuracy: 0.6197\n",
            "Epoch 7/40\n",
            "66/66 [==============================] - 46s 703ms/step - loss: 0.0577 - accuracy: 0.9801 - val_loss: 1.6943 - val_accuracy: 0.5769\n",
            "Epoch 8/40\n",
            "66/66 [==============================] - 46s 702ms/step - loss: 0.0604 - accuracy: 0.9791 - val_loss: 1.7115 - val_accuracy: 0.6111\n",
            "Epoch 9/40\n",
            "66/66 [==============================] - 46s 701ms/step - loss: 0.0807 - accuracy: 0.9753 - val_loss: 1.9160 - val_accuracy: 0.5385\n",
            "Epoch 10/40\n",
            "66/66 [==============================] - 46s 700ms/step - loss: 0.0685 - accuracy: 0.9777 - val_loss: 1.8634 - val_accuracy: 0.5641\n",
            "Epoch 11/40\n",
            "66/66 [==============================] - 46s 698ms/step - loss: 0.0829 - accuracy: 0.9696 - val_loss: 2.1329 - val_accuracy: 0.5128\n",
            "Epoch 12/40\n",
            "66/66 [==============================] - 46s 699ms/step - loss: 0.0752 - accuracy: 0.9739 - val_loss: 1.8549 - val_accuracy: 0.5598\n",
            "Epoch 13/40\n",
            "66/66 [==============================] - 46s 700ms/step - loss: 0.0751 - accuracy: 0.9744 - val_loss: 2.1317 - val_accuracy: 0.5470\n",
            "Epoch 14/40\n",
            "66/66 [==============================] - 47s 705ms/step - loss: 0.0550 - accuracy: 0.9810 - val_loss: 1.9308 - val_accuracy: 0.5812\n",
            "Epoch 15/40\n",
            "66/66 [==============================] - 46s 702ms/step - loss: 0.0557 - accuracy: 0.9820 - val_loss: 1.8433 - val_accuracy: 0.5983\n",
            "Epoch 16/40\n",
            "66/66 [==============================] - 46s 697ms/step - loss: 0.0481 - accuracy: 0.9843 - val_loss: 1.7750 - val_accuracy: 0.6453\n",
            "Epoch 17/40\n",
            "66/66 [==============================] - 46s 700ms/step - loss: 0.0396 - accuracy: 0.9843 - val_loss: 1.6915 - val_accuracy: 0.6111\n",
            "8/8 [==============================] - 1s 185ms/step - loss: 2.2539 - accuracy: 0.5212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0G6w8iWsa1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6015ef26-cebd-4038-d8a3-334a269cd9df"
      },
      "source": [
        "for i, layer in enumerate(base_model.layers):\n",
        "   print(i, layer.name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 input_6\n",
            "1 conv2d_1015\n",
            "2 batch_normalization_1015\n",
            "3 activation_1015\n",
            "4 conv2d_1016\n",
            "5 batch_normalization_1016\n",
            "6 activation_1016\n",
            "7 conv2d_1017\n",
            "8 batch_normalization_1017\n",
            "9 activation_1017\n",
            "10 max_pooling2d_20\n",
            "11 conv2d_1018\n",
            "12 batch_normalization_1018\n",
            "13 activation_1018\n",
            "14 conv2d_1019\n",
            "15 batch_normalization_1019\n",
            "16 activation_1019\n",
            "17 max_pooling2d_21\n",
            "18 conv2d_1023\n",
            "19 batch_normalization_1023\n",
            "20 activation_1023\n",
            "21 conv2d_1021\n",
            "22 conv2d_1024\n",
            "23 batch_normalization_1021\n",
            "24 batch_normalization_1024\n",
            "25 activation_1021\n",
            "26 activation_1024\n",
            "27 average_pooling2d_5\n",
            "28 conv2d_1020\n",
            "29 conv2d_1022\n",
            "30 conv2d_1025\n",
            "31 conv2d_1026\n",
            "32 batch_normalization_1020\n",
            "33 batch_normalization_1022\n",
            "34 batch_normalization_1025\n",
            "35 batch_normalization_1026\n",
            "36 activation_1020\n",
            "37 activation_1022\n",
            "38 activation_1025\n",
            "39 activation_1026\n",
            "40 mixed_5b\n",
            "41 conv2d_1030\n",
            "42 batch_normalization_1030\n",
            "43 activation_1030\n",
            "44 conv2d_1028\n",
            "45 conv2d_1031\n",
            "46 batch_normalization_1028\n",
            "47 batch_normalization_1031\n",
            "48 activation_1028\n",
            "49 activation_1031\n",
            "50 conv2d_1027\n",
            "51 conv2d_1029\n",
            "52 conv2d_1032\n",
            "53 batch_normalization_1027\n",
            "54 batch_normalization_1029\n",
            "55 batch_normalization_1032\n",
            "56 activation_1027\n",
            "57 activation_1029\n",
            "58 activation_1032\n",
            "59 block35_1_mixed\n",
            "60 block35_1_conv\n",
            "61 block35_1\n",
            "62 block35_1_ac\n",
            "63 conv2d_1036\n",
            "64 batch_normalization_1036\n",
            "65 activation_1036\n",
            "66 conv2d_1034\n",
            "67 conv2d_1037\n",
            "68 batch_normalization_1034\n",
            "69 batch_normalization_1037\n",
            "70 activation_1034\n",
            "71 activation_1037\n",
            "72 conv2d_1033\n",
            "73 conv2d_1035\n",
            "74 conv2d_1038\n",
            "75 batch_normalization_1033\n",
            "76 batch_normalization_1035\n",
            "77 batch_normalization_1038\n",
            "78 activation_1033\n",
            "79 activation_1035\n",
            "80 activation_1038\n",
            "81 block35_2_mixed\n",
            "82 block35_2_conv\n",
            "83 block35_2\n",
            "84 block35_2_ac\n",
            "85 conv2d_1042\n",
            "86 batch_normalization_1042\n",
            "87 activation_1042\n",
            "88 conv2d_1040\n",
            "89 conv2d_1043\n",
            "90 batch_normalization_1040\n",
            "91 batch_normalization_1043\n",
            "92 activation_1040\n",
            "93 activation_1043\n",
            "94 conv2d_1039\n",
            "95 conv2d_1041\n",
            "96 conv2d_1044\n",
            "97 batch_normalization_1039\n",
            "98 batch_normalization_1041\n",
            "99 batch_normalization_1044\n",
            "100 activation_1039\n",
            "101 activation_1041\n",
            "102 activation_1044\n",
            "103 block35_3_mixed\n",
            "104 block35_3_conv\n",
            "105 block35_3\n",
            "106 block35_3_ac\n",
            "107 conv2d_1048\n",
            "108 batch_normalization_1048\n",
            "109 activation_1048\n",
            "110 conv2d_1046\n",
            "111 conv2d_1049\n",
            "112 batch_normalization_1046\n",
            "113 batch_normalization_1049\n",
            "114 activation_1046\n",
            "115 activation_1049\n",
            "116 conv2d_1045\n",
            "117 conv2d_1047\n",
            "118 conv2d_1050\n",
            "119 batch_normalization_1045\n",
            "120 batch_normalization_1047\n",
            "121 batch_normalization_1050\n",
            "122 activation_1045\n",
            "123 activation_1047\n",
            "124 activation_1050\n",
            "125 block35_4_mixed\n",
            "126 block35_4_conv\n",
            "127 block35_4\n",
            "128 block35_4_ac\n",
            "129 conv2d_1054\n",
            "130 batch_normalization_1054\n",
            "131 activation_1054\n",
            "132 conv2d_1052\n",
            "133 conv2d_1055\n",
            "134 batch_normalization_1052\n",
            "135 batch_normalization_1055\n",
            "136 activation_1052\n",
            "137 activation_1055\n",
            "138 conv2d_1051\n",
            "139 conv2d_1053\n",
            "140 conv2d_1056\n",
            "141 batch_normalization_1051\n",
            "142 batch_normalization_1053\n",
            "143 batch_normalization_1056\n",
            "144 activation_1051\n",
            "145 activation_1053\n",
            "146 activation_1056\n",
            "147 block35_5_mixed\n",
            "148 block35_5_conv\n",
            "149 block35_5\n",
            "150 block35_5_ac\n",
            "151 conv2d_1060\n",
            "152 batch_normalization_1060\n",
            "153 activation_1060\n",
            "154 conv2d_1058\n",
            "155 conv2d_1061\n",
            "156 batch_normalization_1058\n",
            "157 batch_normalization_1061\n",
            "158 activation_1058\n",
            "159 activation_1061\n",
            "160 conv2d_1057\n",
            "161 conv2d_1059\n",
            "162 conv2d_1062\n",
            "163 batch_normalization_1057\n",
            "164 batch_normalization_1059\n",
            "165 batch_normalization_1062\n",
            "166 activation_1057\n",
            "167 activation_1059\n",
            "168 activation_1062\n",
            "169 block35_6_mixed\n",
            "170 block35_6_conv\n",
            "171 block35_6\n",
            "172 block35_6_ac\n",
            "173 conv2d_1066\n",
            "174 batch_normalization_1066\n",
            "175 activation_1066\n",
            "176 conv2d_1064\n",
            "177 conv2d_1067\n",
            "178 batch_normalization_1064\n",
            "179 batch_normalization_1067\n",
            "180 activation_1064\n",
            "181 activation_1067\n",
            "182 conv2d_1063\n",
            "183 conv2d_1065\n",
            "184 conv2d_1068\n",
            "185 batch_normalization_1063\n",
            "186 batch_normalization_1065\n",
            "187 batch_normalization_1068\n",
            "188 activation_1063\n",
            "189 activation_1065\n",
            "190 activation_1068\n",
            "191 block35_7_mixed\n",
            "192 block35_7_conv\n",
            "193 block35_7\n",
            "194 block35_7_ac\n",
            "195 conv2d_1072\n",
            "196 batch_normalization_1072\n",
            "197 activation_1072\n",
            "198 conv2d_1070\n",
            "199 conv2d_1073\n",
            "200 batch_normalization_1070\n",
            "201 batch_normalization_1073\n",
            "202 activation_1070\n",
            "203 activation_1073\n",
            "204 conv2d_1069\n",
            "205 conv2d_1071\n",
            "206 conv2d_1074\n",
            "207 batch_normalization_1069\n",
            "208 batch_normalization_1071\n",
            "209 batch_normalization_1074\n",
            "210 activation_1069\n",
            "211 activation_1071\n",
            "212 activation_1074\n",
            "213 block35_8_mixed\n",
            "214 block35_8_conv\n",
            "215 block35_8\n",
            "216 block35_8_ac\n",
            "217 conv2d_1078\n",
            "218 batch_normalization_1078\n",
            "219 activation_1078\n",
            "220 conv2d_1076\n",
            "221 conv2d_1079\n",
            "222 batch_normalization_1076\n",
            "223 batch_normalization_1079\n",
            "224 activation_1076\n",
            "225 activation_1079\n",
            "226 conv2d_1075\n",
            "227 conv2d_1077\n",
            "228 conv2d_1080\n",
            "229 batch_normalization_1075\n",
            "230 batch_normalization_1077\n",
            "231 batch_normalization_1080\n",
            "232 activation_1075\n",
            "233 activation_1077\n",
            "234 activation_1080\n",
            "235 block35_9_mixed\n",
            "236 block35_9_conv\n",
            "237 block35_9\n",
            "238 block35_9_ac\n",
            "239 conv2d_1084\n",
            "240 batch_normalization_1084\n",
            "241 activation_1084\n",
            "242 conv2d_1082\n",
            "243 conv2d_1085\n",
            "244 batch_normalization_1082\n",
            "245 batch_normalization_1085\n",
            "246 activation_1082\n",
            "247 activation_1085\n",
            "248 conv2d_1081\n",
            "249 conv2d_1083\n",
            "250 conv2d_1086\n",
            "251 batch_normalization_1081\n",
            "252 batch_normalization_1083\n",
            "253 batch_normalization_1086\n",
            "254 activation_1081\n",
            "255 activation_1083\n",
            "256 activation_1086\n",
            "257 block35_10_mixed\n",
            "258 block35_10_conv\n",
            "259 block35_10\n",
            "260 block35_10_ac\n",
            "261 conv2d_1088\n",
            "262 batch_normalization_1088\n",
            "263 activation_1088\n",
            "264 conv2d_1089\n",
            "265 batch_normalization_1089\n",
            "266 activation_1089\n",
            "267 conv2d_1087\n",
            "268 conv2d_1090\n",
            "269 batch_normalization_1087\n",
            "270 batch_normalization_1090\n",
            "271 activation_1087\n",
            "272 activation_1090\n",
            "273 max_pooling2d_22\n",
            "274 mixed_6a\n",
            "275 conv2d_1092\n",
            "276 batch_normalization_1092\n",
            "277 activation_1092\n",
            "278 conv2d_1093\n",
            "279 batch_normalization_1093\n",
            "280 activation_1093\n",
            "281 conv2d_1091\n",
            "282 conv2d_1094\n",
            "283 batch_normalization_1091\n",
            "284 batch_normalization_1094\n",
            "285 activation_1091\n",
            "286 activation_1094\n",
            "287 block17_1_mixed\n",
            "288 block17_1_conv\n",
            "289 block17_1\n",
            "290 block17_1_ac\n",
            "291 conv2d_1096\n",
            "292 batch_normalization_1096\n",
            "293 activation_1096\n",
            "294 conv2d_1097\n",
            "295 batch_normalization_1097\n",
            "296 activation_1097\n",
            "297 conv2d_1095\n",
            "298 conv2d_1098\n",
            "299 batch_normalization_1095\n",
            "300 batch_normalization_1098\n",
            "301 activation_1095\n",
            "302 activation_1098\n",
            "303 block17_2_mixed\n",
            "304 block17_2_conv\n",
            "305 block17_2\n",
            "306 block17_2_ac\n",
            "307 conv2d_1100\n",
            "308 batch_normalization_1100\n",
            "309 activation_1100\n",
            "310 conv2d_1101\n",
            "311 batch_normalization_1101\n",
            "312 activation_1101\n",
            "313 conv2d_1099\n",
            "314 conv2d_1102\n",
            "315 batch_normalization_1099\n",
            "316 batch_normalization_1102\n",
            "317 activation_1099\n",
            "318 activation_1102\n",
            "319 block17_3_mixed\n",
            "320 block17_3_conv\n",
            "321 block17_3\n",
            "322 block17_3_ac\n",
            "323 conv2d_1104\n",
            "324 batch_normalization_1104\n",
            "325 activation_1104\n",
            "326 conv2d_1105\n",
            "327 batch_normalization_1105\n",
            "328 activation_1105\n",
            "329 conv2d_1103\n",
            "330 conv2d_1106\n",
            "331 batch_normalization_1103\n",
            "332 batch_normalization_1106\n",
            "333 activation_1103\n",
            "334 activation_1106\n",
            "335 block17_4_mixed\n",
            "336 block17_4_conv\n",
            "337 block17_4\n",
            "338 block17_4_ac\n",
            "339 conv2d_1108\n",
            "340 batch_normalization_1108\n",
            "341 activation_1108\n",
            "342 conv2d_1109\n",
            "343 batch_normalization_1109\n",
            "344 activation_1109\n",
            "345 conv2d_1107\n",
            "346 conv2d_1110\n",
            "347 batch_normalization_1107\n",
            "348 batch_normalization_1110\n",
            "349 activation_1107\n",
            "350 activation_1110\n",
            "351 block17_5_mixed\n",
            "352 block17_5_conv\n",
            "353 block17_5\n",
            "354 block17_5_ac\n",
            "355 conv2d_1112\n",
            "356 batch_normalization_1112\n",
            "357 activation_1112\n",
            "358 conv2d_1113\n",
            "359 batch_normalization_1113\n",
            "360 activation_1113\n",
            "361 conv2d_1111\n",
            "362 conv2d_1114\n",
            "363 batch_normalization_1111\n",
            "364 batch_normalization_1114\n",
            "365 activation_1111\n",
            "366 activation_1114\n",
            "367 block17_6_mixed\n",
            "368 block17_6_conv\n",
            "369 block17_6\n",
            "370 block17_6_ac\n",
            "371 conv2d_1116\n",
            "372 batch_normalization_1116\n",
            "373 activation_1116\n",
            "374 conv2d_1117\n",
            "375 batch_normalization_1117\n",
            "376 activation_1117\n",
            "377 conv2d_1115\n",
            "378 conv2d_1118\n",
            "379 batch_normalization_1115\n",
            "380 batch_normalization_1118\n",
            "381 activation_1115\n",
            "382 activation_1118\n",
            "383 block17_7_mixed\n",
            "384 block17_7_conv\n",
            "385 block17_7\n",
            "386 block17_7_ac\n",
            "387 conv2d_1120\n",
            "388 batch_normalization_1120\n",
            "389 activation_1120\n",
            "390 conv2d_1121\n",
            "391 batch_normalization_1121\n",
            "392 activation_1121\n",
            "393 conv2d_1119\n",
            "394 conv2d_1122\n",
            "395 batch_normalization_1119\n",
            "396 batch_normalization_1122\n",
            "397 activation_1119\n",
            "398 activation_1122\n",
            "399 block17_8_mixed\n",
            "400 block17_8_conv\n",
            "401 block17_8\n",
            "402 block17_8_ac\n",
            "403 conv2d_1124\n",
            "404 batch_normalization_1124\n",
            "405 activation_1124\n",
            "406 conv2d_1125\n",
            "407 batch_normalization_1125\n",
            "408 activation_1125\n",
            "409 conv2d_1123\n",
            "410 conv2d_1126\n",
            "411 batch_normalization_1123\n",
            "412 batch_normalization_1126\n",
            "413 activation_1123\n",
            "414 activation_1126\n",
            "415 block17_9_mixed\n",
            "416 block17_9_conv\n",
            "417 block17_9\n",
            "418 block17_9_ac\n",
            "419 conv2d_1128\n",
            "420 batch_normalization_1128\n",
            "421 activation_1128\n",
            "422 conv2d_1129\n",
            "423 batch_normalization_1129\n",
            "424 activation_1129\n",
            "425 conv2d_1127\n",
            "426 conv2d_1130\n",
            "427 batch_normalization_1127\n",
            "428 batch_normalization_1130\n",
            "429 activation_1127\n",
            "430 activation_1130\n",
            "431 block17_10_mixed\n",
            "432 block17_10_conv\n",
            "433 block17_10\n",
            "434 block17_10_ac\n",
            "435 conv2d_1132\n",
            "436 batch_normalization_1132\n",
            "437 activation_1132\n",
            "438 conv2d_1133\n",
            "439 batch_normalization_1133\n",
            "440 activation_1133\n",
            "441 conv2d_1131\n",
            "442 conv2d_1134\n",
            "443 batch_normalization_1131\n",
            "444 batch_normalization_1134\n",
            "445 activation_1131\n",
            "446 activation_1134\n",
            "447 block17_11_mixed\n",
            "448 block17_11_conv\n",
            "449 block17_11\n",
            "450 block17_11_ac\n",
            "451 conv2d_1136\n",
            "452 batch_normalization_1136\n",
            "453 activation_1136\n",
            "454 conv2d_1137\n",
            "455 batch_normalization_1137\n",
            "456 activation_1137\n",
            "457 conv2d_1135\n",
            "458 conv2d_1138\n",
            "459 batch_normalization_1135\n",
            "460 batch_normalization_1138\n",
            "461 activation_1135\n",
            "462 activation_1138\n",
            "463 block17_12_mixed\n",
            "464 block17_12_conv\n",
            "465 block17_12\n",
            "466 block17_12_ac\n",
            "467 conv2d_1140\n",
            "468 batch_normalization_1140\n",
            "469 activation_1140\n",
            "470 conv2d_1141\n",
            "471 batch_normalization_1141\n",
            "472 activation_1141\n",
            "473 conv2d_1139\n",
            "474 conv2d_1142\n",
            "475 batch_normalization_1139\n",
            "476 batch_normalization_1142\n",
            "477 activation_1139\n",
            "478 activation_1142\n",
            "479 block17_13_mixed\n",
            "480 block17_13_conv\n",
            "481 block17_13\n",
            "482 block17_13_ac\n",
            "483 conv2d_1144\n",
            "484 batch_normalization_1144\n",
            "485 activation_1144\n",
            "486 conv2d_1145\n",
            "487 batch_normalization_1145\n",
            "488 activation_1145\n",
            "489 conv2d_1143\n",
            "490 conv2d_1146\n",
            "491 batch_normalization_1143\n",
            "492 batch_normalization_1146\n",
            "493 activation_1143\n",
            "494 activation_1146\n",
            "495 block17_14_mixed\n",
            "496 block17_14_conv\n",
            "497 block17_14\n",
            "498 block17_14_ac\n",
            "499 conv2d_1148\n",
            "500 batch_normalization_1148\n",
            "501 activation_1148\n",
            "502 conv2d_1149\n",
            "503 batch_normalization_1149\n",
            "504 activation_1149\n",
            "505 conv2d_1147\n",
            "506 conv2d_1150\n",
            "507 batch_normalization_1147\n",
            "508 batch_normalization_1150\n",
            "509 activation_1147\n",
            "510 activation_1150\n",
            "511 block17_15_mixed\n",
            "512 block17_15_conv\n",
            "513 block17_15\n",
            "514 block17_15_ac\n",
            "515 conv2d_1152\n",
            "516 batch_normalization_1152\n",
            "517 activation_1152\n",
            "518 conv2d_1153\n",
            "519 batch_normalization_1153\n",
            "520 activation_1153\n",
            "521 conv2d_1151\n",
            "522 conv2d_1154\n",
            "523 batch_normalization_1151\n",
            "524 batch_normalization_1154\n",
            "525 activation_1151\n",
            "526 activation_1154\n",
            "527 block17_16_mixed\n",
            "528 block17_16_conv\n",
            "529 block17_16\n",
            "530 block17_16_ac\n",
            "531 conv2d_1156\n",
            "532 batch_normalization_1156\n",
            "533 activation_1156\n",
            "534 conv2d_1157\n",
            "535 batch_normalization_1157\n",
            "536 activation_1157\n",
            "537 conv2d_1155\n",
            "538 conv2d_1158\n",
            "539 batch_normalization_1155\n",
            "540 batch_normalization_1158\n",
            "541 activation_1155\n",
            "542 activation_1158\n",
            "543 block17_17_mixed\n",
            "544 block17_17_conv\n",
            "545 block17_17\n",
            "546 block17_17_ac\n",
            "547 conv2d_1160\n",
            "548 batch_normalization_1160\n",
            "549 activation_1160\n",
            "550 conv2d_1161\n",
            "551 batch_normalization_1161\n",
            "552 activation_1161\n",
            "553 conv2d_1159\n",
            "554 conv2d_1162\n",
            "555 batch_normalization_1159\n",
            "556 batch_normalization_1162\n",
            "557 activation_1159\n",
            "558 activation_1162\n",
            "559 block17_18_mixed\n",
            "560 block17_18_conv\n",
            "561 block17_18\n",
            "562 block17_18_ac\n",
            "563 conv2d_1164\n",
            "564 batch_normalization_1164\n",
            "565 activation_1164\n",
            "566 conv2d_1165\n",
            "567 batch_normalization_1165\n",
            "568 activation_1165\n",
            "569 conv2d_1163\n",
            "570 conv2d_1166\n",
            "571 batch_normalization_1163\n",
            "572 batch_normalization_1166\n",
            "573 activation_1163\n",
            "574 activation_1166\n",
            "575 block17_19_mixed\n",
            "576 block17_19_conv\n",
            "577 block17_19\n",
            "578 block17_19_ac\n",
            "579 conv2d_1168\n",
            "580 batch_normalization_1168\n",
            "581 activation_1168\n",
            "582 conv2d_1169\n",
            "583 batch_normalization_1169\n",
            "584 activation_1169\n",
            "585 conv2d_1167\n",
            "586 conv2d_1170\n",
            "587 batch_normalization_1167\n",
            "588 batch_normalization_1170\n",
            "589 activation_1167\n",
            "590 activation_1170\n",
            "591 block17_20_mixed\n",
            "592 block17_20_conv\n",
            "593 block17_20\n",
            "594 block17_20_ac\n",
            "595 conv2d_1175\n",
            "596 batch_normalization_1175\n",
            "597 activation_1175\n",
            "598 conv2d_1171\n",
            "599 conv2d_1173\n",
            "600 conv2d_1176\n",
            "601 batch_normalization_1171\n",
            "602 batch_normalization_1173\n",
            "603 batch_normalization_1176\n",
            "604 activation_1171\n",
            "605 activation_1173\n",
            "606 activation_1176\n",
            "607 conv2d_1172\n",
            "608 conv2d_1174\n",
            "609 conv2d_1177\n",
            "610 batch_normalization_1172\n",
            "611 batch_normalization_1174\n",
            "612 batch_normalization_1177\n",
            "613 activation_1172\n",
            "614 activation_1174\n",
            "615 activation_1177\n",
            "616 max_pooling2d_23\n",
            "617 mixed_7a\n",
            "618 conv2d_1179\n",
            "619 batch_normalization_1179\n",
            "620 activation_1179\n",
            "621 conv2d_1180\n",
            "622 batch_normalization_1180\n",
            "623 activation_1180\n",
            "624 conv2d_1178\n",
            "625 conv2d_1181\n",
            "626 batch_normalization_1178\n",
            "627 batch_normalization_1181\n",
            "628 activation_1178\n",
            "629 activation_1181\n",
            "630 block8_1_mixed\n",
            "631 block8_1_conv\n",
            "632 block8_1\n",
            "633 block8_1_ac\n",
            "634 conv2d_1183\n",
            "635 batch_normalization_1183\n",
            "636 activation_1183\n",
            "637 conv2d_1184\n",
            "638 batch_normalization_1184\n",
            "639 activation_1184\n",
            "640 conv2d_1182\n",
            "641 conv2d_1185\n",
            "642 batch_normalization_1182\n",
            "643 batch_normalization_1185\n",
            "644 activation_1182\n",
            "645 activation_1185\n",
            "646 block8_2_mixed\n",
            "647 block8_2_conv\n",
            "648 block8_2\n",
            "649 block8_2_ac\n",
            "650 conv2d_1187\n",
            "651 batch_normalization_1187\n",
            "652 activation_1187\n",
            "653 conv2d_1188\n",
            "654 batch_normalization_1188\n",
            "655 activation_1188\n",
            "656 conv2d_1186\n",
            "657 conv2d_1189\n",
            "658 batch_normalization_1186\n",
            "659 batch_normalization_1189\n",
            "660 activation_1186\n",
            "661 activation_1189\n",
            "662 block8_3_mixed\n",
            "663 block8_3_conv\n",
            "664 block8_3\n",
            "665 block8_3_ac\n",
            "666 conv2d_1191\n",
            "667 batch_normalization_1191\n",
            "668 activation_1191\n",
            "669 conv2d_1192\n",
            "670 batch_normalization_1192\n",
            "671 activation_1192\n",
            "672 conv2d_1190\n",
            "673 conv2d_1193\n",
            "674 batch_normalization_1190\n",
            "675 batch_normalization_1193\n",
            "676 activation_1190\n",
            "677 activation_1193\n",
            "678 block8_4_mixed\n",
            "679 block8_4_conv\n",
            "680 block8_4\n",
            "681 block8_4_ac\n",
            "682 conv2d_1195\n",
            "683 batch_normalization_1195\n",
            "684 activation_1195\n",
            "685 conv2d_1196\n",
            "686 batch_normalization_1196\n",
            "687 activation_1196\n",
            "688 conv2d_1194\n",
            "689 conv2d_1197\n",
            "690 batch_normalization_1194\n",
            "691 batch_normalization_1197\n",
            "692 activation_1194\n",
            "693 activation_1197\n",
            "694 block8_5_mixed\n",
            "695 block8_5_conv\n",
            "696 block8_5\n",
            "697 block8_5_ac\n",
            "698 conv2d_1199\n",
            "699 batch_normalization_1199\n",
            "700 activation_1199\n",
            "701 conv2d_1200\n",
            "702 batch_normalization_1200\n",
            "703 activation_1200\n",
            "704 conv2d_1198\n",
            "705 conv2d_1201\n",
            "706 batch_normalization_1198\n",
            "707 batch_normalization_1201\n",
            "708 activation_1198\n",
            "709 activation_1201\n",
            "710 block8_6_mixed\n",
            "711 block8_6_conv\n",
            "712 block8_6\n",
            "713 block8_6_ac\n",
            "714 conv2d_1203\n",
            "715 batch_normalization_1203\n",
            "716 activation_1203\n",
            "717 conv2d_1204\n",
            "718 batch_normalization_1204\n",
            "719 activation_1204\n",
            "720 conv2d_1202\n",
            "721 conv2d_1205\n",
            "722 batch_normalization_1202\n",
            "723 batch_normalization_1205\n",
            "724 activation_1202\n",
            "725 activation_1205\n",
            "726 block8_7_mixed\n",
            "727 block8_7_conv\n",
            "728 block8_7\n",
            "729 block8_7_ac\n",
            "730 conv2d_1207\n",
            "731 batch_normalization_1207\n",
            "732 activation_1207\n",
            "733 conv2d_1208\n",
            "734 batch_normalization_1208\n",
            "735 activation_1208\n",
            "736 conv2d_1206\n",
            "737 conv2d_1209\n",
            "738 batch_normalization_1206\n",
            "739 batch_normalization_1209\n",
            "740 activation_1206\n",
            "741 activation_1209\n",
            "742 block8_8_mixed\n",
            "743 block8_8_conv\n",
            "744 block8_8\n",
            "745 block8_8_ac\n",
            "746 conv2d_1211\n",
            "747 batch_normalization_1211\n",
            "748 activation_1211\n",
            "749 conv2d_1212\n",
            "750 batch_normalization_1212\n",
            "751 activation_1212\n",
            "752 conv2d_1210\n",
            "753 conv2d_1213\n",
            "754 batch_normalization_1210\n",
            "755 batch_normalization_1213\n",
            "756 activation_1210\n",
            "757 activation_1213\n",
            "758 block8_9_mixed\n",
            "759 block8_9_conv\n",
            "760 block8_9\n",
            "761 block8_9_ac\n",
            "762 conv2d_1215\n",
            "763 batch_normalization_1215\n",
            "764 activation_1215\n",
            "765 conv2d_1216\n",
            "766 batch_normalization_1216\n",
            "767 activation_1216\n",
            "768 conv2d_1214\n",
            "769 conv2d_1217\n",
            "770 batch_normalization_1214\n",
            "771 batch_normalization_1217\n",
            "772 activation_1214\n",
            "773 activation_1217\n",
            "774 block8_10_mixed\n",
            "775 block8_10_conv\n",
            "776 block8_10\n",
            "777 conv_7b\n",
            "778 conv_7b_bn\n",
            "779 conv_7b_ac\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ko-nqgPslXb"
      },
      "source": [
        "#Let all the upper layers train on the new data\n",
        "#experimented with many different layer combinations\n",
        "for layer in model.layers[:673]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[673:]:\n",
        "   layer.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t0-Rjpps6rX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e3f6d1c-2073-4234-b645-1b8b146fa6f7"
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=nadam, metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=35, callbacks=monitor_val_acc)\n",
        "score = model.evaluate(test_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            "66/66 [==============================] - 35s 528ms/step - loss: 0.0626 - accuracy: 0.9872 - val_loss: 1.4692 - val_accuracy: 0.6453\n",
            "Epoch 2/35\n",
            "66/66 [==============================] - 32s 489ms/step - loss: 0.0228 - accuracy: 0.9929 - val_loss: 1.4472 - val_accuracy: 0.6453\n",
            "Epoch 3/35\n",
            "66/66 [==============================] - 32s 486ms/step - loss: 0.0133 - accuracy: 0.9943 - val_loss: 1.4610 - val_accuracy: 0.6453\n",
            "Epoch 4/35\n",
            "66/66 [==============================] - 32s 483ms/step - loss: 0.0130 - accuracy: 0.9938 - val_loss: 1.4925 - val_accuracy: 0.6410\n",
            "Epoch 5/35\n",
            "66/66 [==============================] - 32s 485ms/step - loss: 0.0132 - accuracy: 0.9929 - val_loss: 1.5213 - val_accuracy: 0.6453\n",
            "Epoch 6/35\n",
            "66/66 [==============================] - 32s 485ms/step - loss: 0.0121 - accuracy: 0.9948 - val_loss: 1.5470 - val_accuracy: 0.6325\n",
            "Epoch 7/35\n",
            "66/66 [==============================] - 32s 485ms/step - loss: 0.0104 - accuracy: 0.9953 - val_loss: 1.5828 - val_accuracy: 0.6282\n",
            "Epoch 8/35\n",
            "66/66 [==============================] - 32s 483ms/step - loss: 0.0094 - accuracy: 0.9943 - val_loss: 1.6109 - val_accuracy: 0.6325\n",
            "Epoch 9/35\n",
            "66/66 [==============================] - 32s 482ms/step - loss: 0.0076 - accuracy: 0.9957 - val_loss: 1.6027 - val_accuracy: 0.6538\n",
            "Epoch 10/35\n",
            "66/66 [==============================] - 32s 481ms/step - loss: 0.0106 - accuracy: 0.9948 - val_loss: 1.6917 - val_accuracy: 0.6239\n",
            "Epoch 11/35\n",
            "66/66 [==============================] - 32s 486ms/step - loss: 0.0106 - accuracy: 0.9934 - val_loss: 1.6415 - val_accuracy: 0.6410\n",
            "Epoch 12/35\n",
            "66/66 [==============================] - 32s 483ms/step - loss: 0.0075 - accuracy: 0.9967 - val_loss: 1.6364 - val_accuracy: 0.6197\n",
            "Epoch 13/35\n",
            "66/66 [==============================] - 32s 484ms/step - loss: 0.0084 - accuracy: 0.9953 - val_loss: 1.7057 - val_accuracy: 0.6368\n",
            "Epoch 14/35\n",
            "66/66 [==============================] - 32s 483ms/step - loss: 0.0077 - accuracy: 0.9943 - val_loss: 1.7146 - val_accuracy: 0.6325\n",
            "Epoch 15/35\n",
            "66/66 [==============================] - 32s 482ms/step - loss: 0.0075 - accuracy: 0.9953 - val_loss: 1.7096 - val_accuracy: 0.6282\n",
            "Epoch 16/35\n",
            "66/66 [==============================] - 32s 481ms/step - loss: 0.0076 - accuracy: 0.9948 - val_loss: 1.7114 - val_accuracy: 0.6282\n",
            "Epoch 17/35\n",
            "66/66 [==============================] - 32s 483ms/step - loss: 0.0088 - accuracy: 0.9953 - val_loss: 1.7285 - val_accuracy: 0.6282\n",
            "8/8 [==============================] - 1s 137ms/step - loss: 2.1580 - accuracy: 0.5720\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onyXSXgY20-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e89ab8-a8e1-448a-af94-b3d6a3710e77"
      },
      "source": [
        "model.save('/content/drive/My Drive/plant_inceptionresnet_model')\n",
        "#model.save('/content/drive/My Drive/plant_inceptionresnet_model_h5.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/plant_inceptionresnet_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpu5sTgpcxUq"
      },
      "source": [
        "## Try Image Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bCPr-bufHwa"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mbh9evVp36-"
      },
      "source": [
        "def preprocess_gen(image):\n",
        "    #resized_image = tf.image.resize(image, [224, 224])\n",
        "    final_image = inception_resnet_v2.preprocess_input(image)\n",
        "    return final_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdRQiUEFczzl",
        "outputId": "d3f954fb-0d8e-48e0-b223-dfae78d90722"
      },
      "source": [
        "#experimented with different parameters \n",
        "train_datagen = ImageDataGenerator(\n",
        "        featurewise_center=True,\n",
        "        featurewise_std_normalization=True,\n",
        "        rotation_range=45,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        preprocessing_function=preprocess_gen,\n",
        "        validation_split=0.1)\n",
        "\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_gen)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        subset='training')\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        subset='validation')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2118 images belonging to 27 classes.\n",
            "Found 222 images belonging to 27 classes.\n",
            "Found 236 images belonging to 27 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOZl21lef5PD"
      },
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=nadam, metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtP8skTfdjqV",
        "outputId": "cd6b2588-b61f-4c2b-92ff-895a757bdf15"
      },
      "source": [
        "history = model.fit(train_generator, epochs=25, validation_data=val_generator, callbacks=monitor_val_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "67/67 [==============================] - 92s 1s/step - loss: 1.0186 - accuracy: 0.7025 - val_loss: 1.3650 - val_accuracy: 0.6351\n",
            "Epoch 2/25\n",
            "67/67 [==============================] - 88s 1s/step - loss: 0.7364 - accuracy: 0.7748 - val_loss: 1.0559 - val_accuracy: 0.6892\n",
            "Epoch 3/25\n",
            "67/67 [==============================] - 87s 1s/step - loss: 0.5528 - accuracy: 0.8239 - val_loss: 1.0942 - val_accuracy: 0.6892\n",
            "Epoch 4/25\n",
            "67/67 [==============================] - 88s 1s/step - loss: 0.5019 - accuracy: 0.8390 - val_loss: 1.0908 - val_accuracy: 0.6667\n",
            "Epoch 5/25\n",
            "67/67 [==============================] - 87s 1s/step - loss: 0.4269 - accuracy: 0.8598 - val_loss: 1.1198 - val_accuracy: 0.6847\n",
            "Epoch 6/25\n",
            "67/67 [==============================] - 86s 1s/step - loss: 0.3909 - accuracy: 0.8749 - val_loss: 1.0275 - val_accuracy: 0.7297\n",
            "Epoch 7/25\n",
            "67/67 [==============================] - 86s 1s/step - loss: 0.3208 - accuracy: 0.8876 - val_loss: 1.1485 - val_accuracy: 0.7072\n",
            "Epoch 8/25\n",
            "67/67 [==============================] - 86s 1s/step - loss: 0.2842 - accuracy: 0.9093 - val_loss: 1.1102 - val_accuracy: 0.7117\n",
            "Epoch 9/25\n",
            "67/67 [==============================] - 86s 1s/step - loss: 0.2771 - accuracy: 0.9056 - val_loss: 1.3392 - val_accuracy: 0.6441\n",
            "Epoch 10/25\n",
            "67/67 [==============================] - 86s 1s/step - loss: 0.2373 - accuracy: 0.9263 - val_loss: 1.1713 - val_accuracy: 0.6802\n",
            "Epoch 11/25\n",
            "67/67 [==============================] - 86s 1s/step - loss: 0.2362 - accuracy: 0.9230 - val_loss: 1.0559 - val_accuracy: 0.7297\n",
            "Epoch 12/25\n",
            "67/67 [==============================] - 85s 1s/step - loss: 0.2030 - accuracy: 0.9348 - val_loss: 1.3138 - val_accuracy: 0.6577\n",
            "Epoch 13/25\n",
            "67/67 [==============================] - 86s 1s/step - loss: 0.1885 - accuracy: 0.9372 - val_loss: 1.1712 - val_accuracy: 0.6757\n",
            "Epoch 14/25\n",
            "67/67 [==============================] - 86s 1s/step - loss: 0.1890 - accuracy: 0.9405 - val_loss: 1.1044 - val_accuracy: 0.6982\n",
            "Epoch 15/25\n",
            "67/67 [==============================] - 86s 1s/step - loss: 0.1703 - accuracy: 0.9495 - val_loss: 1.1283 - val_accuracy: 0.6847\n",
            "Epoch 16/25\n",
            "67/67 [==============================] - 86s 1s/step - loss: 0.1661 - accuracy: 0.9490 - val_loss: 1.3769 - val_accuracy: 0.6667\n",
            "Epoch 17/25\n",
            "67/67 [==============================] - 85s 1s/step - loss: 0.1574 - accuracy: 0.9537 - val_loss: 1.2654 - val_accuracy: 0.6892\n",
            "Epoch 18/25\n",
            "67/67 [==============================] - 85s 1s/step - loss: 0.1429 - accuracy: 0.9514 - val_loss: 1.3163 - val_accuracy: 0.6396\n",
            "Epoch 19/25\n",
            "67/67 [==============================] - 85s 1s/step - loss: 0.1294 - accuracy: 0.9580 - val_loss: 1.4584 - val_accuracy: 0.6441\n",
            "Epoch 20/25\n",
            "67/67 [==============================] - 85s 1s/step - loss: 0.1228 - accuracy: 0.9599 - val_loss: 1.6368 - val_accuracy: 0.6171\n",
            "Epoch 21/25\n",
            "67/67 [==============================] - 85s 1s/step - loss: 0.1369 - accuracy: 0.9566 - val_loss: 1.2735 - val_accuracy: 0.6757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D6EKSVr03D1",
        "outputId": "68c65686-dcf9-408c-cacf-5757b99ad008"
      },
      "source": [
        "model.evaluate(test_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 6s 704ms/step - loss: 2.3289 - accuracy: 0.5720\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.328927755355835, 0.5720338821411133]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEH_VPrL2VJp"
      },
      "source": [
        "### Xception Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQjGE78ENJDj"
      },
      "source": [
        "from keras.applications import Xception\n",
        "from keras.applications import xception"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH0CtLL7RsBQ"
      },
      "source": [
        "def preprocess_xception(image, label):\n",
        "    #resized_image = tf.image.resize(image, [224, 224])\n",
        "    final_image = xception.preprocess_input(image)\n",
        "    return final_image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q_idbVvSLkr"
      },
      "source": [
        "train_set = train_ds.map(preprocess_xception).prefetch(1)\n",
        "valid_set = val_ds.map(preprocess_xception).prefetch(1)\n",
        "test_set = test_ds.map(preprocess_xception).prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtFfvmW6NJnd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47238cf3-8390-4eba-ecc1-da0a96a15a9d"
      },
      "source": [
        "tf.random.set_seed(123)\n",
        "np.random.seed(123)\n",
        "base_model = Xception(include_top=False, \n",
        "                               weights='imagenet', \n",
        "                               pooling='avg')\n",
        "\n",
        "output = keras.layers.Dense(len(class_names), activation='softmax')(base_model.output)\n",
        "model = keras.models.Model(inputs=base_model.inputs, outputs=output)\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", \n",
        "    optimizer=\"nadam\", \n",
        "    metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, epochs=25, validation_data=valid_set, callbacks=monitor_val_acc)\n",
        "score = model.evaluate(test_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "66/66 [==============================] - 48s 727ms/step - loss: 2.0203 - accuracy: 0.3989 - val_loss: 3.2045 - val_accuracy: 0.2863\n",
            "Epoch 2/25\n",
            "66/66 [==============================] - 47s 709ms/step - loss: 1.0225 - accuracy: 0.6652 - val_loss: 2.4207 - val_accuracy: 0.4017\n",
            "Epoch 3/25\n",
            "66/66 [==============================] - 47s 714ms/step - loss: 0.6364 - accuracy: 0.8010 - val_loss: 3.1575 - val_accuracy: 0.3120\n",
            "Epoch 4/25\n",
            "66/66 [==============================] - 47s 712ms/step - loss: 0.4170 - accuracy: 0.8751 - val_loss: 1.9267 - val_accuracy: 0.4786\n",
            "Epoch 5/25\n",
            "66/66 [==============================] - 47s 712ms/step - loss: 0.2720 - accuracy: 0.9155 - val_loss: 2.0267 - val_accuracy: 0.5726\n",
            "Epoch 6/25\n",
            "66/66 [==============================] - 47s 707ms/step - loss: 0.1763 - accuracy: 0.9487 - val_loss: 2.7033 - val_accuracy: 0.4274\n",
            "Epoch 7/25\n",
            "66/66 [==============================] - 47s 706ms/step - loss: 0.2268 - accuracy: 0.9245 - val_loss: 1.9234 - val_accuracy: 0.5342\n",
            "Epoch 8/25\n",
            "66/66 [==============================] - 47s 709ms/step - loss: 0.2024 - accuracy: 0.9326 - val_loss: 2.5123 - val_accuracy: 0.4786\n",
            "Epoch 9/25\n",
            "66/66 [==============================] - 47s 711ms/step - loss: 0.3036 - accuracy: 0.8989 - val_loss: 2.9457 - val_accuracy: 0.4316\n",
            "Epoch 10/25\n",
            "66/66 [==============================] - 47s 716ms/step - loss: 0.2612 - accuracy: 0.9160 - val_loss: 2.4218 - val_accuracy: 0.4573\n",
            "Epoch 11/25\n",
            "66/66 [==============================] - 47s 714ms/step - loss: 0.1635 - accuracy: 0.9397 - val_loss: 2.9648 - val_accuracy: 0.5043\n",
            "Epoch 12/25\n",
            "66/66 [==============================] - 47s 713ms/step - loss: 0.1227 - accuracy: 0.9554 - val_loss: 2.8287 - val_accuracy: 0.5256\n",
            "Epoch 13/25\n",
            "66/66 [==============================] - 47s 714ms/step - loss: 0.0853 - accuracy: 0.9739 - val_loss: 1.9273 - val_accuracy: 0.5897\n",
            "Epoch 14/25\n",
            "66/66 [==============================] - 47s 713ms/step - loss: 0.0750 - accuracy: 0.9687 - val_loss: 2.3704 - val_accuracy: 0.5598\n",
            "Epoch 15/25\n",
            "66/66 [==============================] - 47s 714ms/step - loss: 0.0999 - accuracy: 0.9658 - val_loss: 2.1050 - val_accuracy: 0.5726\n",
            "Epoch 16/25\n",
            "66/66 [==============================] - 47s 716ms/step - loss: 0.1223 - accuracy: 0.9554 - val_loss: 2.9251 - val_accuracy: 0.4444\n",
            "Epoch 17/25\n",
            "66/66 [==============================] - 47s 715ms/step - loss: 0.1545 - accuracy: 0.9478 - val_loss: 2.9749 - val_accuracy: 0.4444\n",
            "Epoch 18/25\n",
            "66/66 [==============================] - 47s 713ms/step - loss: 0.1285 - accuracy: 0.9573 - val_loss: 2.8570 - val_accuracy: 0.5171\n",
            "Epoch 19/25\n",
            "66/66 [==============================] - 47s 716ms/step - loss: 0.1313 - accuracy: 0.9554 - val_loss: 2.7220 - val_accuracy: 0.5299\n",
            "Epoch 20/25\n",
            "66/66 [==============================] - 47s 716ms/step - loss: 0.1509 - accuracy: 0.9473 - val_loss: 4.5758 - val_accuracy: 0.3675\n",
            "Epoch 21/25\n",
            "66/66 [==============================] - 47s 719ms/step - loss: 0.1286 - accuracy: 0.9544 - val_loss: 2.4834 - val_accuracy: 0.5171\n",
            "Epoch 22/25\n",
            "66/66 [==============================] - 47s 718ms/step - loss: 0.1236 - accuracy: 0.9596 - val_loss: 2.8188 - val_accuracy: 0.4957\n",
            "8/8 [==============================] - 1s 162ms/step - loss: 3.4793 - accuracy: 0.4195\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbWkUgakWMmG"
      },
      "source": [
        "With data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBVf6KNNWOCH"
      },
      "source": [
        "def preprocess_gen_x(image):\n",
        "    #resized_image = tf.image.resize(image, [224, 224])\n",
        "    final_image = xception.preprocess_input(image)\n",
        "    return final_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCldWjfZWXqc",
        "outputId": "50715471-9630-4bbc-e6a0-deee500ad696"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        featurewise_center=True,\n",
        "        featurewise_std_normalization=True,\n",
        "        rotation_range=45,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        preprocessing_function=preprocess_gen_x,\n",
        "        validation_split=0.1)\n",
        "\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_gen_x)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        subset='training')\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        subset='validation')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2118 images belonging to 27 classes.\n",
            "Found 222 images belonging to 27 classes.\n",
            "Found 236 images belonging to 27 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whDl1kvEWjw5",
        "outputId": "71d614af-38b7-4cbe-ec82-c79b713f924c"
      },
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=nadam, metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(train_generator, epochs=25, validation_data=val_generator, callbacks=monitor_val_acc)\n",
        "\n",
        "model.evaluate(test_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "67/67 [==============================] - 100s 1s/step - loss: 1.0998 - accuracy: 0.6808 - val_loss: 1.6066 - val_accuracy: 0.6126\n",
            "Epoch 2/25\n",
            "67/67 [==============================] - 96s 1s/step - loss: 0.7439 - accuracy: 0.7672 - val_loss: 0.9146 - val_accuracy: 0.7207\n",
            "Epoch 3/25\n",
            "67/67 [==============================] - 97s 1s/step - loss: 0.5748 - accuracy: 0.8069 - val_loss: 1.0278 - val_accuracy: 0.6847\n",
            "Epoch 4/25\n",
            "67/67 [==============================] - 101s 2s/step - loss: 0.4805 - accuracy: 0.8437 - val_loss: 0.8583 - val_accuracy: 0.7207\n",
            "Epoch 5/25\n",
            "67/67 [==============================] - 102s 2s/step - loss: 0.4244 - accuracy: 0.8617 - val_loss: 0.8976 - val_accuracy: 0.7568\n",
            "Epoch 6/25\n",
            "67/67 [==============================] - 99s 1s/step - loss: 0.3640 - accuracy: 0.8824 - val_loss: 0.8498 - val_accuracy: 0.7387\n",
            "Epoch 7/25\n",
            "67/67 [==============================] - 99s 1s/step - loss: 0.3464 - accuracy: 0.8782 - val_loss: 0.9005 - val_accuracy: 0.7252\n",
            "Epoch 8/25\n",
            "67/67 [==============================] - 100s 1s/step - loss: 0.2892 - accuracy: 0.9051 - val_loss: 0.8389 - val_accuracy: 0.7387\n",
            "Epoch 9/25\n",
            "67/67 [==============================] - 100s 1s/step - loss: 0.2731 - accuracy: 0.9056 - val_loss: 1.0625 - val_accuracy: 0.6982\n",
            "Epoch 10/25\n",
            "67/67 [==============================] - 101s 2s/step - loss: 0.2498 - accuracy: 0.9150 - val_loss: 0.9648 - val_accuracy: 0.7568\n",
            "Epoch 11/25\n",
            "67/67 [==============================] - 101s 2s/step - loss: 0.2299 - accuracy: 0.9230 - val_loss: 1.0850 - val_accuracy: 0.7252\n",
            "Epoch 12/25\n",
            "67/67 [==============================] - 101s 2s/step - loss: 0.1970 - accuracy: 0.9282 - val_loss: 0.9364 - val_accuracy: 0.7568\n",
            "Epoch 13/25\n",
            "67/67 [==============================] - 101s 2s/step - loss: 0.2014 - accuracy: 0.9306 - val_loss: 0.9341 - val_accuracy: 0.7658\n",
            "Epoch 14/25\n",
            "67/67 [==============================] - 102s 2s/step - loss: 0.1845 - accuracy: 0.9353 - val_loss: 0.8311 - val_accuracy: 0.7613\n",
            "Epoch 15/25\n",
            "67/67 [==============================] - 101s 2s/step - loss: 0.1784 - accuracy: 0.9433 - val_loss: 0.8907 - val_accuracy: 0.7613\n",
            "Epoch 16/25\n",
            "67/67 [==============================] - 102s 2s/step - loss: 0.1592 - accuracy: 0.9476 - val_loss: 1.0092 - val_accuracy: 0.7387\n",
            "Epoch 17/25\n",
            "67/67 [==============================] - 102s 2s/step - loss: 0.1247 - accuracy: 0.9561 - val_loss: 1.1741 - val_accuracy: 0.7117\n",
            "Epoch 18/25\n",
            "67/67 [==============================] - 102s 2s/step - loss: 0.1376 - accuracy: 0.9448 - val_loss: 0.8873 - val_accuracy: 0.7613\n",
            "Epoch 19/25\n",
            "67/67 [==============================] - 102s 2s/step - loss: 0.1312 - accuracy: 0.9528 - val_loss: 1.1148 - val_accuracy: 0.6937\n",
            "Epoch 20/25\n",
            "67/67 [==============================] - 102s 2s/step - loss: 0.1405 - accuracy: 0.9476 - val_loss: 1.0406 - val_accuracy: 0.7297\n",
            "Epoch 21/25\n",
            "67/67 [==============================] - 102s 2s/step - loss: 0.1103 - accuracy: 0.9608 - val_loss: 1.0477 - val_accuracy: 0.7568\n",
            "Epoch 22/25\n",
            "67/67 [==============================] - 102s 2s/step - loss: 0.1213 - accuracy: 0.9556 - val_loss: 0.9563 - val_accuracy: 0.7568\n",
            "Epoch 23/25\n",
            "67/67 [==============================] - 102s 2s/step - loss: 0.1201 - accuracy: 0.9646 - val_loss: 0.9338 - val_accuracy: 0.7342\n",
            "Epoch 24/25\n",
            "67/67 [==============================] - 102s 2s/step - loss: 0.0934 - accuracy: 0.9655 - val_loss: 0.9272 - val_accuracy: 0.7432\n",
            "Epoch 25/25\n",
            "67/67 [==============================] - 101s 2s/step - loss: 0.0932 - accuracy: 0.9651 - val_loss: 1.0061 - val_accuracy: 0.7342\n",
            "8/8 [==============================] - 6s 708ms/step - loss: 2.3551 - accuracy: 0.5932\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.3551173210144043, 0.5932203531265259]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z934ChAthSfD",
        "outputId": "fd5bd7ae-d051-418c-8b4d-5a042ed38b79"
      },
      "source": [
        "model.save('/content/drive/My Drive/plant_xception_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/plant_xception_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLP27ItwLdsx"
      },
      "source": [
        "### Transfer Model to GCS Bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLEOTavkhak2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2364dfc-c1d0-42af-cad8-9e7d40a7e992"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "project_id = 'velvety-transit-295121'\n",
        "!gcloud config set project {project_id}\n",
        "!gsutil ls\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "gs://staging.velvety-transit-295121.appspot.com/\n",
            "gs://sys6016_plant_image/\n",
            "gs://us.artifacts.velvety-transit-295121.appspot.com/\n",
            "gs://velvety-transit-295121.appspot.com/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSFea1szL6XY",
        "outputId": "6d142658-aa6d-4ee5-bab3-95688b281626"
      },
      "source": [
        "bucket_name = 'sys6016_plant_image/plant_xception_model'\n",
        "\n",
        "!gsutil -m cp -r /content/drive/My\\ Drive/plant_xception_model/* gs://{bucket_name}/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file:///content/drive/My Drive/plant_xception_model/saved_model.pb [Content-Type=application/octet-stream]...\n",
            "/ [0/3 files][    0.0 B/241.9 MiB]   0% Done                                    \rCopying file:///content/drive/My Drive/plant_xception_model/variables/variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
            "/ [0/3 files][    0.0 B/241.9 MiB]   0% Done                                    \rCopying file:///content/drive/My Drive/plant_xception_model/variables/variables.index [Content-Type=application/octet-stream]...\n",
            "/ [0/3 files][    0.0 B/241.9 MiB]   0% Done                                    \r==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "/ [3/3 files][241.9 MiB/241.9 MiB] 100% Done                                    \n",
            "Operation completed over 3 objects/241.9 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8gFcQSRMymX",
        "outputId": "26a0336e-da07-4c52-a319-880e46cc8f3b"
      },
      "source": [
        "bucket_name = 'sys6016_plant_image/plant_inceptionresnet_model'\n",
        "\n",
        "!gsutil -m cp -r /content/drive/My\\ Drive/plant_inceptionresnet_model/* gs://{bucket_name}/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file:///content/drive/My Drive/plant_inceptionresnet_model/variables/variables.index [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/drive/My Drive/plant_inceptionresnet_model/saved_model.pb [Content-Type=application/octet-stream]...\n",
            "/ [0/3 files][    0.0 B/635.0 MiB]   0% Done                                    \r/ [0/3 files][    0.0 B/635.0 MiB]   0% Done                                    \rCopying file:///content/drive/My Drive/plant_inceptionresnet_model/variables/variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
            "/ [0/3 files][    0.0 B/635.0 MiB]   0% Done                                    \r==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\ [3/3 files][635.0 MiB/635.0 MiB] 100% Done                                    \n",
            "Operation completed over 3 objects/635.0 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}